# -*- coding: utf-8 -*-
"""Autoencoders_assignement_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Kyveli-tsioli/hello-world/blob/main/Autoencoders_assignement_2.ipynb
"""

!pip install -U tensorflow_datasets

import tensorflow as tf

# Import TensorFlow Datasets
import tensorflow_datasets as tfds
tfds.disable_progress_bar()


import math
import numpy as np
from numpy import random
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
import random
from google.colab import files

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import copy
from mpl_toolkits.axes_grid1 import ImageGrid

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images
print(np.shape(train_images))
print(np.shape(test_images))

#create train set, validation set, test set
train_set=train_images[0:50000,:,:]
print("train set",np.shape(train_set))
valid_set=train_images[50000:60000,:,:]
print("validation set",np.shape(valid_set))
test_set=test_images
print("test set",np.shape(test_images))

type(train_set)
print(type(valid_set))
print(type(test_set))
print(type(train_labels))

#label sets
train_set_labels=train_labels[0:50000]
print("train set labels",np.shape(train_set_labels))
valid_set_labels=train_labels[50000:60000]
print("validation set labels",np.shape(valid_set_labels))
test_set_labels=test_labels
print("test set labels",np.shape(test_set_labels))

train_set[0,:,:]
np.shape(train_set[0,:,:])

dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)

class_names = metadata.features['label'].names
#not included with the dataset, store them here to use later when plotting images 


print("Class names: {}".format(class_names))

class_names

plt.figure()
plt.imshow(train_set[10],cmap=plt.cm.binary)
plt.colorbar()
plt.grid(False)
plt.show()

#preprocessing-normalisation

train_set=train_set/255.0
valid_set=valid_set/255.0
test_set=test_set/255.0

#scale these values to a range of 0 to 1 before feeding them to the neural network 
#divide the values by 255

#display the first 25 images from the training set
plt.figure(figsize=(10,10))
for i in range(25):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(train_set[i],cmap=plt.cm.binary)
  plt.xlabel(class_names[train_set_labels[i]])
plt.show()

np.shape(train_set)

np.shape(valid_set)

#Construct the function noise which removes one or two quadrants of the image
def noise(x):
  corrupted_images=[]
  for i in range(np.shape(x)[0]):
    x_copy=np.copy(x[i])
    x_offset=random.randint(0,14)
    y_offset=random.randint(0,14)
    for k in range(x_offset,x_offset+14):
        for m in range(y_offset,y_offset+14):
            x_copy[k,m]=0
    corrupted_images.append(x_copy)
    
  return np.array(corrupted_images)

#trial: we apply the noise function only for a subset of images 
train_set=train_set[0:50000]
train_set_labels=train_set_labels[0:50000]
valid_set=train_set[0:200]
valid_set_labels=valid_set_labels[0:200]

#take the noise version of each and every picture in the training set
train_set_corr=noise(train_set)
valid_set_corr=noise(valid_set)
test_set_corr=noise(test_set)
print(test_set_corr.shape)
print(type(test_set_corr))

plt.figure()
plt.imshow(train_set[0],cmap=plt.cm.binary)
plt.colorbar()
plt.grid(False)
plt.show()

np.shape(train_set_corr)

plt.figure()
plt.imshow(train_set_corr[0],cmap=plt.cm.binary)
plt.colorbar()
plt.grid(False)
plt.show()

plt.figure(figsize=(10,10))
for i in range(25):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(train_set_corr[i],cmap=plt.cm.binary)
  plt.xlabel(class_names[train_set_labels[i]])
plt.show()

#make them 2D arrays
x_train=[]
for i in range(train_set.shape[0]):
  x_train.append(train_set[i].flatten())
x_train=np.array(x_train)
print(x_train)
print(np.shape(x_train))

x_valid=[]
for i in range(valid_set.shape[0]):
  x_valid.append(valid_set[i].flatten())
x_valid=np.array(x_valid)
print(x_valid)
print(np.shape(x_valid))

x_test=[]
for i in range(test_set.shape[0]):
  x_test.append(test_set[i].flatten())
x_test=np.array(x_test)
print(x_test)
print(np.shape(x_test))

x_train_corr=[]
for i in range(train_set_corr.shape[0]):
  x_train_corr.append(train_set_corr[i].flatten())
x_train_corr=np.array(x_train_corr)
print(x_train_corr)
print(np.shape(x_train_corr))


x_valid_corr=[]
for i in range(valid_set_corr.shape[0]):
  x_valid_corr.append(valid_set_corr[i].flatten())
x_valid_corr=np.array(x_valid_corr)
print(x_valid_corr)
print(np.shape(x_valid_corr))

x_test_corr=[]
for i in range(test_set_corr.shape[0]):
  x_test_corr.append(test_set_corr[i].flatten())
x_test_corr=np.array(x_test_corr)
print(x_test_corr)
print(np.shape(x_test_corr))

#convert data to torch tensors
#train set tensors
train_set_tensor=torch.tensor(x_train,dtype=torch.float)
train_labels_tensor=torch.tensor(train_set_labels,dtype=torch.float)


#validation set tensors
valid_set_tensor=torch.tensor(x_valid,dtype=torch.float)
valid_labels_tensor=torch.tensor(valid_set_labels,dtype=torch.float)

#test set tensors
test_set_tensor=torch.tensor(x_test,dtype=torch.float)
test_labels_tensor=torch.tensor(test_set_labels,dtype=torch.float)

#corrupted tensors for the 3 sets
train_set_corr_tensor=torch.tensor(x_train_corr,dtype=torch.float)
valid_set_corr_tensor=torch.tensor(x_valid_corr,dtype=torch.float)
test_set_corr_tensor=torch.tensor(x_test_corr,dtype=torch.float)

print(train_set_tensor.size())
print("train set tensor size:",train_set_tensor.shape)
print("valid set tensor size:",valid_set_tensor.shape)
print("test set tensor size:",test_set_tensor.shape)
print("corrupted train",train_set_corr_tensor.shape)
print("corrupted valid",valid_set_corr_tensor.shape)
print("corrupted test", test_set_corr_tensor.shape)

#use the training set tesdor and the train labels tensor to create a TensorDataset as well
#as a DataLoader that allows to load batches of our data during each training iteration

batch_size=256
trainset=torch.utils.data.TensorDataset(train_set_tensor,train_set_corr_tensor) #train_set_corr
trainloader=torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2)


#same for validation set
validset=torch.utils.data.TensorDataset(valid_set_tensor,valid_set_corr_tensor) #corrupted
validloader=torch.utils.data.DataLoader(validset,batch_size=batch_size,shuffle=False,num_workers=2)

class autoencoder(nn.Module):
  def __init__(self):
    super(autoencoder, self).__init__()

    #encoder
    self.input_size=28*28   #to be defined outside the class according to the size of the set 
    self.enc1=nn.Linear(self.input_size,500)
    self.enc2=nn.Linear(500,230) 
    self.enc3=nn.Linear(230,126)
    #self.enc4=nn.Linear(64,32)
    #self.enc5=nn.Linear(32,16)
 


    #decoder
    self.dec1=nn.Linear(126,230)
    self.dec2=nn.Linear(230,500)
    self.dec3=nn.Linear(500,self.input_size)
    #self.dec4=nn.Linear(128,256)
    #self.dec5=nn.Linear(256,self.input_size)


  def forward(self,x):
    x = torch.sigmoid(self.enc1(x))
    x = torch.sigmoid(self.enc2(x))
    x = torch.sigmoid(self.enc3(x))
    #x = torch.sigmoid(self.enc4(x))
    #x = torch.sigmoid(self.enc5(x))

    x = torch.sigmoid(self.dec1(x))
    x = torch.sigmoid(self.dec2(x))
    x = torch.sigmoid(self.dec3(x))
    #x = torch.sigmoid(self.dec4(x))
    #x = torch.sigmoid(self.dec5(x))
    return x

#create an instance of the autoencoder class 
model=autoencoder()
print(model)

criterion=nn.MSELoss()
learning_rate=0.01
epochs=100
#optimizer=optim.SGD(model.parameters(), lr=learning_rate)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

def get_device():
    if torch.cuda.is_available():
        device = 'cuda:0'
    else:
        device = 'cpu'
    return device

#training 
def training(model,train_loader,valid_loader,epochs):
  
  
  loss_epochs=[]
  valid_loss_epochs=[]
  epoch_list=[]
  test_loss_epochs=[]

  for epoch in range(epochs):  # loop over the dataset multiple times
    
    num_of_batches=0
    epoch_list.append(epoch)
    batch_loss=[]

    
    for i,data in enumerate (train_loader,0): #batch in trainloader, for each of the epochs we split the training data into mini batches using the DataLoader 
        
        img_real, img_corr = data #get batch #img stands for the corrupted images while labels stands for the real images 
        
        optimizer.zero_grad()  #clear the gradients at each batch step, in pytorch we need to set the gradients to zero before starting to do backpropagation because it accummulates the gradients on subsequent backward passes 
       
        output=model(img_corr)  #forward pass: pass batch, compute predicted outputs  
        
        
        loss=criterion(output,img_real)
        
        loss.backward()   #backward pass: compute gradient of loss with respect to model parameters 
        optimizer.step()  #update weights, performs a parameter update based on the current gradient, otherwise backward wouldnt update the weights without the optimizer, it would just compute gradients  
          
        batch_loss.append(loss.item()) 
        #batch_loss.append(loss.item()*batch_size) 
        num_of_batches=+1
 
    
    loss_epochs.append(np.mean(np.array(batch_loss))) #The average of the batch losses will give you an estimate of the “epoch loss” during training

   
    #validation set metrics
    valid_img_real,valid_img_corr=valid_set_tensor,valid_set_corr_tensor
    output_valid=model(valid_img_corr)
    valid_loss=criterion(output_valid,valid_img_real)
    valid_loss_epochs.append(valid_loss.item())
    #valid_loss_epochs.append(valid_loss.item()*batch_size)

    #test set metrics: needed for the last question
    test_img_real,test_img_corr=test_set_tensor,test_set_corr_tensor
    output_test=model(test_img_corr)
    test_loss=criterion(output_test,test_img_real)
    test_loss_epochs.append(test_loss.item())


    #validation loss and training loss over the epochs
    print(epoch," validation loss",valid_loss_epochs[-1],"training loss ",loss_epochs[-1])

    
    
  training_loss_plot=plt.plot(epoch_list,loss_epochs,label='Training loss')
  validation_loss_plot=plt.plot(epoch_list,valid_loss_epochs,label='Validation loss')
  plt.legend()
  plt.title("Loss per epochs")
  plt.show()
  


 

  #final question: provide a set of 32 images in their original, noisy and decoded form from the test set
  test_images_list=[]
  for k in range(4):
    image_list=[]
    for i in range(8):
    
      index = random.randrange(0, 10000)
      print("index of the image", index)
    
      #real test set image
      print("original image from the test set")
      reshaped_test_real=test_img_real.view(10000,28,28) 
      reshaped_test_real=reshaped_test_real.cpu().detach().numpy() #use detach when its an output that requires grad
      plt.figure()
      plt.imshow(reshaped_test_real[index,:,:])
      image_list.append(reshaped_test_real[index,:,:])
  
    
      #noisy test set image
      print("corrupted image from the test set")
      reshaped_test_corr=test_img_corr.view(10000,28,28) #validation set corrupted image
      reshaped_test_corr=reshaped_test_corr.cpu().detach().numpy() #use detach when its an output that requires grad
      image_list.append(reshaped_test_corr[index,:,:])
      

      #denoised test set image
      print("denoised image from the test set")
      reshaped_test_output=output_test.view(10000,28,28) #validation set corrupted image
      reshaped_test_output=reshaped_test_output.cpu().detach().numpy() #use detach when its an output that requires grad
      image_list.append(reshaped_test_output[index,:,:])
      
    test_images_list.append(image_list)
    print("length",len(image_list))
   print(np.array(test_images_list).shape)
   for l in range(len(test_images_list)):
    fig = plt.figure(figsize=(20., 20.))
    grid = ImageGrid(fig, 111,  
                  nrows_ncols=(8, 3),  
                  axes_pad=0.1,  
                  )

    for ax, im in zip(grid, test_images_list[l]):
      # Iterating over the grid returns the Axes.
      ax.imshow(im)

    plt.show()

 

trial=training(model,trainloader,validloader,epochs)

